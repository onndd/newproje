{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Proje Kurulumu ve Güncelleme (Colab venv ile)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if IN_COLAB:\n",
    "    repo_url = \"https://github.com/onndd/newproje.git\"\n",
    "    project_dir = \"/content/newproje\"\n",
    "    if not os.path.exists(project_dir):\n",
    "        !git clone {repo_url} {project_dir}\n",
    "    %cd {project_dir}\n",
    "    !git pull\n",
    "\n",
    "    venv_dir = \"/content/jetx-venv\"\n",
    "    if not os.path.exists(venv_dir):\n",
    "        !python -m venv {venv_dir}\n",
    "    pip_cmd = f\"{venv_dir}/bin/pip\"\n",
    "    python_cmd = f\"{venv_dir}/bin/python\"\n",
    "    !{pip_cmd} install --upgrade pip\n",
    "\n",
    "    # Çekirdek paketler\n",
    "    !{pip_cmd} install numpy==1.26.2 pandas==2.1.4 matplotlib==3.8.2 seaborn==0.13.0 joblib==1.3.2 scikit-learn==1.5.2\n",
    "\n",
    "    # Model çekirdekleri (dep çekmeden)\n",
    "    !{pip_cmd} install --no-deps catboost==1.2.8 lightgbm==4.6.0 hmmlearn==0.3.2\n",
    "\n",
    "    # Diğerleri\n",
    "    !{pip_cmd} install streamlit==1.29.0 optuna==3.5.0 stable-baselines3==2.2.1 shimmy==1.3.0 gymnasium==0.29.1 \"gymnasium[box2d]\" tensorflow==2.16.1\n",
    "\n",
    "    sys.path.insert(0, os.path.join(venv_dir, 'lib', 'python3.12', 'site-packages'))\n",
    "else:\n",
    "    project_dir = os.getcwd()\n",
    "    sys.path.append(project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Veri Yükleme ve 3'lü Ayrım (3-Way Split)\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jetx_project.data_loader import load_data\n",
    "from jetx_project.config import DB_PATH\n",
    "\n",
    "print(\"Veri yükleniyor...\")\n",
    "try:\n",
    "    df = load_data(DB_PATH)\n",
    "    print(f\"Toplam {len(df)} veri yüklendi.\")\n",
    "    \n",
    "    # --- 3-WAY SPLIT STRATEJİSİ ---\n",
    "    # 1. Base Train (%70): Temel modelleri eğitmek için.\n",
    "    # 2. Meta Train (%15): Temel modellerin tahminlerini alıp Meta-Learner'ı eğitmek için.\n",
    "    # 3. Test (%15): Final simülasyon ve değerlendirme için (Hiçbir model burayı görmez).\n",
    "    \n",
    "    total_len = len(df)\n",
    "    base_train_end = int(total_len * 0.70)\n",
    "    meta_train_end = int(total_len * 0.85)\n",
    "    \n",
    "    print(f\"--- Veri Ayrımı ---\")\n",
    "    print(f\"Base Train: 0 - {base_train_end} ({base_train_end} örnek)\")\n",
    "    print(f\"Meta Train: {base_train_end} - {meta_train_end} ({meta_train_end - base_train_end} örnek)\")\n",
    "    print(f\"Test (Sim): {meta_train_end} - {total_len} ({total_len - meta_train_end} örnek)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Hata: {e}\")\n",
    "    print(\"Lütfen jetx.db dosyasının proje klasöründe olduğundan emin olun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5. Anomaly Detector (The Shield) Eğitimi\n",
    "from jetx_project.model_anomaly import train_anomaly_detector, save_anomaly_detector\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Anomaly Detector (The Shield) eğitiliyor...\")\n",
    "    # Base Train verisiyle eğit\n",
    "    train_values_anomaly = df['value'].values[:base_train_end]\n",
    "    anomaly_model = train_anomaly_detector(train_values_anomaly)\n",
    "    save_anomaly_detector(anomaly_model, output_dir='models')\n",
    "    print(\"Anomaly Detector tamamlandı.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Psikolojik Modeller (HMM) Eğitimi\n",
    "print(\"Psikolojik Modeller (HMM) eğitiliyor...\")\n",
    "from jetx_project.model_hmm import train_categorical_hmm, predict_categorical_hmm_states_causal, save_hmm_model\n",
    "\n",
    "if 'df' in locals() and len(df) > 0:\n",
    "    # Sadece Base Train verisiyle eğit\n",
    "    train_values = df['value'].values[:base_train_end]\n",
    "    \n",
    "    # HMM Eğitimi (Categorical)\n",
    "    hmm_model, hmm_map, hmm_bins = train_categorical_hmm(train_values)\n",
    "    save_hmm_model(hmm_model, hmm_map, bins=hmm_bins, output_dir='models')\n",
    "    \n",
    "    # Tüm veri seti için durum tahmini (Feature olarak kullanılacak)\n",
    "    # CAUSAL PREDICTION: Geleceği görmeden, kayan pencere ile tahmin et.\n",
    "    all_values = df['value'].values\n",
    "    hmm_states_mapped = predict_categorical_hmm_states_causal(hmm_model, all_values, hmm_map, bins=hmm_bins, window_size=200)\n",
    "    \n",
    "    print(\"HMM Durumları (Causal) tüm veri için belirlendi.\")\n",
    "    print(f\"Durum Dağılımı: {np.bincount(hmm_states_mapped)}\")\n",
    "else:\n",
    "    print(\"Veri yok, HMM eğitilemedi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. OPTUNA İLE HİPERPARAMETRE OPTİMİZASYONU (Base Train Üzerinde)\n",
    "from jetx_project.optimization import optimize_catboost, optimize_lightgbm\n",
    "from jetx_project.model_a import prepare_model_a_data\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Optimizasyon için veri hazırlanıyor...\")\n",
    "    # Tüm veriyi hazırla ama sadece Base Train kısmını kullan\n",
    "    X, y_p15, y_p3, y_x = prepare_model_a_data(df['value'].values, hmm_states_mapped)\n",
    "    \n",
    "    # Base Train Maskesi\n",
    "    # X indexleri orijinal df indexleriyle uyumlu olmalı (prepare_model_a_data koruyor mu?)\n",
    "    # prepare_model_a_data içinde valid_mask var, indexleri resetlemiyor sanırım.\n",
    "    # Güvenli olmak için iloc kullanalım, ancak X'in uzunluğu len(df) - 1 (shift yüzünden).\n",
    "    \n",
    "    # X'in satır sayısı len(df)-1 civarı. \n",
    "    # Base Train End indexine kadar olanları alacağız.\n",
    "    \n",
    "    # Basitçe: İlk %70'lik dilim.\n",
    "    limit_idx = base_train_end\n",
    "    \n",
    "    # X bir DataFrame, indexleri kontrol edelim\n",
    "    X_train_opt = X[X.index < limit_idx]\n",
    "    # y numpy array, maske ile filtrele\n",
    "    mask_opt = (X.index < limit_idx)\n",
    "    y_p15_train_opt = y_p15[mask_opt]\n",
    "    \n",
    "    print(f\"Optimizasyon Veri Seti: {len(X_train_opt)} satır\")\n",
    "    \n",
    "    print(\"--- CatBoost Optimizasyonu Başlıyor (GPU) ---\")\n",
    "    best_params_catboost = optimize_catboost(X_train_opt, y_p15_train_opt, n_trials=20)\n",
    "    print(\"Bulunan En İyi Parametreler:\", best_params_catboost)\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model A (CatBoost) Eğitimi (Base Train)\n",
    "from jetx_project.model_a import train_model_a, save_models\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Model A (CatBoost) eğitiliyor...\")\n",
    "    \n",
    "    # Sadece Base Train verisini kullan\n",
    "    limit_idx = base_train_end\n",
    "    mask_train = (X.index < limit_idx)\n",
    "    \n",
    "    X_train_base = X[mask_train]\n",
    "    y_p15_base = y_p15[mask_train]\n",
    "    y_p3_base = y_p3[mask_train]\n",
    "    y_x_base = y_x[mask_train]\n",
    "    \n",
    "    # train_model_a kendi içinde %15 validasyon ayırır (Base Train'in %15'i)\n",
    "    modelA_p15, modelA_p3, modelA_x = train_model_a(X_train_base, y_p15_base, y_p3_base, y_x_base)\n",
    "    save_models(modelA_p15, modelA_p3, modelA_x, output_dir='models')\n",
    "    print(\"Model A tamamlandı.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model B (k-NN) Eğitimi (Base Train)\n",
    "from jetx_project.model_b import build_memory, train_model_b, save_memory\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Model B (Hafıza) oluşturuluyor...\")\n",
    "    \n",
    "    # Sadece Base Train verisiyle hafıza oluştur\n",
    "    train_values_b = df['value'].values[:base_train_end]\n",
    "    patterns, targets = build_memory(train_values_b)\n",
    "    \n",
    "    nbrs, pca = train_model_b(patterns)\n",
    "    \n",
    "    save_memory(nbrs, pca, patterns, targets, output_dir='models')\n",
    "    print(\"Model B tamamlandı.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model C (LSTM) Eğitimi (Base Train)\n",
    "from jetx_project.model_lstm import train_model_lstm, save_lstm_models\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Model C (LSTM) eğitiliyor...\")\n",
    "    \n",
    "    # Sadece Base Train verisi\n",
    "    train_values_c = df['value'].values[:base_train_end]\n",
    "    \n",
    "    modelC_p15, modelC_p3, scaler_lstm = train_model_lstm(train_values_c)\n",
    "    save_lstm_models(modelC_p15, modelC_p3, scaler_lstm, output_dir='models')\n",
    "    print(\"Model C tamamlandı.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Model D (LightGBM) Eğitimi (Base Train)\n",
    "from jetx_project.model_lightgbm import train_model_lightgbm, save_lightgbm_models\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Model D (LightGBM) eğitiliyor...\")\n",
    "    modelD_p15, modelD_p3 = train_model_lightgbm(X_train_base, y_p15_base, y_p3_base)\n",
    "    save_lightgbm_models(modelD_p15, modelD_p3, output_dir='models')\n",
    "    print(\"Model D tamamlandı.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Model E (MLP) Eğitimi (Base Train)\n",
    "from jetx_project.model_mlp import train_model_mlp, save_mlp_models\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Model E (MLP) eğitiliyor...\")\n",
    "    modelE_p15, modelE_p3, mlp_cols = train_model_mlp(X_train_base, y_p15_base, y_p3_base)\n",
    "    save_mlp_models(modelE_p15, modelE_p3, mlp_cols, output_dir='models')\n",
    "    print(\"Model E tamamlandı.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. Model F (Transformer - The Attention) Eğitimi (Base Train)\n",
    "from jetx_project.model_transformer import train_model_transformer, save_transformer_models\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Model F (Transformer) eğitiliyor...\")\n",
    "    # Base Train verisi\n",
    "    train_values_f = df['value'].values[:base_train_end]\n",
    "    \n",
    "    model_transformer, scaler_transformer = train_model_transformer(train_values_f)\n",
    "    save_transformer_models(model_transformer, scaler_transformer, output_dir='models')\n",
    "    print(\"Model F (Transformer) tamamlandı.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5. ENSEMBLE STACKING (Meta-Learner) Eğitimi (Meta Train)\n",
    "from jetx_project.ensemble import prepare_meta_features, train_meta_learner, save_meta_learner, predict_meta\n",
    "from jetx_project.model_lstm import create_sequences\n",
    "\n",
    "if 'df' in locals() and len(df) > 2000:\n",
    "    print(\"--- Ensemble Stacking (Meta-Learner) Eğitiliyor ---\")\n",
    "    \n",
    "    meta_start_idx = base_train_end\n",
    "    meta_end_idx = meta_train_end\n",
    "    \n",
    "    print(f\"Meta Train Aralığı: {meta_start_idx} - {meta_end_idx}\")\n",
    "    \n",
    "    # 1. Meta Train Seti İçin Tahminleri Topla\n",
    "    \n",
    "    # Model A (CatBoost)\n",
    "    X_meta, _, _, _ = prepare_model_a_data(df['value'].values, hmm_states_mapped, start_index=meta_start_idx)\n",
    "    X_meta = X_meta[X_meta.index < meta_end_idx]\n",
    "    \n",
    "    # Hedefler\n",
    "    y_meta_true_15 = (df['value'].values[meta_start_idx+1 : meta_end_idx+1] >= 1.5).astype(int)\n",
    "    \n",
    "    # Uzunluk Kontrolü\n",
    "    min_len = min(len(X_meta), len(y_meta_true_15))\n",
    "    X_meta = X_meta.iloc[:min_len]\n",
    "    y_meta_true_15 = y_meta_true_15[:min_len]\n",
    "    \n",
    "    print(f\"Meta Train Örnek Sayısı: {min_len}\")\n",
    "    \n",
    "    preds_a_meta = modelA_p15.predict_proba(X_meta)[:, 1]\n",
    "    \n",
    "    # Model B (k-NN)\n",
    "    preds_b_meta = []\n",
    "    from jetx_project.model_b import create_pattern_vector, predict_model_b\n",
    "    pca_obj = locals().get('pca', None) \n",
    "    \n",
    "    for idx in X_meta.index:\n",
    "        pat = create_pattern_vector(df['value'].values, idx)\n",
    "        if pat is not None:\n",
    "            p15, _, _ = predict_model_b(nbrs, pca_obj, targets, pat)\n",
    "            preds_b_meta.append(p15)\n",
    "        else:\n",
    "            preds_b_meta.append(0.5)\n",
    "    preds_b_meta = np.array(preds_b_meta)\n",
    "            \n",
    "    # Model C (LSTM)\n",
    "    seq_len = 200\n",
    "    lstm_start_offset = X_meta.index[0] - seq_len\n",
    "    lstm_end_offset = X_meta.index[-1]\n",
    "    \n",
    "    meta_values_extended = df['value'].values[lstm_start_offset : lstm_end_offset + 1]\n",
    "    meta_values_scaled = scaler_lstm.transform(meta_values_extended.reshape(-1, 1))\n",
    "    \n",
    "    X_lstm_meta, _, _, _ = create_sequences(meta_values_scaled, seq_len)\n",
    "    preds_c_meta = modelC_p15.predict(X_lstm_meta).flatten()\n",
    "    \n",
    "    # Model D (LightGBM)\n",
    "    preds_d_meta = modelD_p15.predict_proba(X_meta)[:, 1]\n",
    "    \n",
    "    # Model E (MLP)\n",
    "    X_meta_mlp = X_meta[mlp_cols]\n",
    "    preds_e_meta = modelE_p15.predict_proba(X_meta_mlp)[:, 1]\n",
    "\n",
    "    # Model F (Transformer)\n",
    "    meta_values_scaled_trans = scaler_transformer.transform(meta_values_extended.reshape(-1, 1))\n",
    "    X_trans_meta, _, _, _ = create_sequences(meta_values_scaled_trans, seq_len)\n",
    "    preds_f_meta = model_transformer.predict(X_trans_meta)\n",
    "    preds_f_meta_p15 = preds_f_meta[0].flatten()\n",
    "    \n",
    "    # HMM States\n",
    "    hmm_meta = hmm_states_mapped[X_meta.index]\n",
    "    \n",
    "    # 2. Meta-Features Hazırla\n",
    "    final_len = min(len(preds_a_meta), len(preds_b_meta), len(preds_c_meta), len(preds_d_meta), len(preds_e_meta), len(preds_f_meta_p15))\n",
    "    \n",
    "    meta_values_raw = df['value'].values[meta_start_idx : meta_end_idx]\n",
    "    meta_values_raw = meta_values_raw[:final_len]\n",
    "    \n",
    "    meta_X_train = prepare_meta_features(\n",
    "        preds_a_meta[:final_len], \n",
    "        preds_b_meta[:final_len], \n",
    "        preds_c_meta[:final_len], \n",
    "        preds_d_meta[:final_len], \n",
    "        preds_e_meta[:final_len], \n",
    "        hmm_meta[:final_len],\n",
    "        values=meta_values_raw,\n",
    "        preds_transformer=preds_f_meta_p15[:final_len]\n",
    "    )\n",
    "    y_meta_train = y_meta_true_15[:final_len]\n",
    "    \n",
    "    # 3. Eğit\n",
    "    meta_model, meta_scaler = train_meta_learner(meta_X_train, y_meta_train)\n",
    "    save_meta_learner(meta_model, meta_scaler, output_dir='models')\n",
    "    \n",
    "    print(\"Meta-Learner eğitildi ve kaydedildi.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.8. RL Agent (The Strategist) Eğitimi\n",
    "from jetx_project.model_rl import train_rl_agent, save_rl_agent\n",
    "\n",
    "if 'meta_model' in locals():\n",
    "    print(\"RL Agent (The Strategist) eğitiliyor...\")\n",
    "    \n",
    "    # Meta-Learner tahminlerini alalım\n",
    "    meta_probs = predict_meta(meta_model, meta_scaler, meta_X_train)\n",
    "    \n",
    "    # DataFrame oluştur\n",
    "    rl_train_df = pd.DataFrame({\n",
    "        'value': meta_values_raw, # Gerçek değerler\n",
    "        'prob_1.5': meta_probs,\n",
    "        'hmm_state': hmm_meta[:final_len] # HMM durumları\n",
    "    })\n",
    "    \n",
    "    rl_agent = train_rl_agent(rl_train_df)\n",
    "    save_rl_agent(rl_agent, output_dir='models')\n",
    "    print(\"RL Agent tamamlandı.\")\n",
    "else:\n",
    "    print(\"Meta-Learner hazır değil.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. BÜYÜK FİNAL: Simülasyon (Test Seti)\n",
    "import matplotlib.pyplot as plt\n",
    "from jetx_project.simulation import run_simulation\n",
    "from jetx_project.evaluation import detailed_evaluation\n",
    "from jetx_project.ensemble import predict_meta_safe\n",
    "from jetx_project.model_rl import predict_action\n",
    "from jetx_project.model_anomaly import check_anomaly\n",
    "\n",
    "if 'df' in locals() and len(df) > 2000:\n",
    "    print(\"--- BÜYÜK FİNAL: Simülasyon Başlıyor (Test Seti) ---\")\n",
    "    \n",
    "    test_start_idx = meta_train_end\n",
    "    test_df = df.iloc[test_start_idx:].copy()\n",
    "    \n",
    "    print(f\"Test Seti Başlangıç: {test_start_idx}, Uzunluk: {len(test_df)}\")\n",
    "    \n",
    "    # --- Tahminleri Topla ---\n",
    "    print(\"Modeller tahmin ediyor...\")\n",
    "    X_test_a, _, _, _ = prepare_model_a_data(df['value'].values, hmm_states_mapped, start_index=test_start_idx)\n",
    "    sim_df = test_df.iloc[:-1].copy()\n",
    "    min_len = min(len(X_test_a), len(sim_df))\n",
    "    X_test_a = X_test_a.iloc[:min_len]\n",
    "    sim_df = sim_df.iloc[:min_len]\n",
    "    \n",
    "    # Model A\n",
    "    sim_df['prob_A_1.5'] = modelA_p15.predict_proba(X_test_a)[:, 1]\n",
    "    sim_df['prob_A_3.0'] = modelA_p3.predict_proba(X_test_a)[:, 1]\n",
    "    \n",
    "    # Model C (LSTM)\n",
    "    seq_len = 200\n",
    "    lstm_start = test_start_idx - seq_len\n",
    "    test_values_extended = df['value'].values[lstm_start:]\n",
    "    test_values_scaled = scaler_lstm.transform(test_values_extended.reshape(-1, 1))\n",
    "    X_lstm, _, _, _ = create_sequences(test_values_scaled, seq_len)\n",
    "    sim_df['prob_C_1.5'] = modelC_p15.predict(X_lstm).flatten()[:min_len]\n",
    "    \n",
    "    # Model D (LightGBM)\n",
    "    sim_df['prob_D_1.5'] = modelD_p15.predict_proba(X_test_a)[:, 1]\n",
    "    \n",
    "    # Model E (MLP)\n",
    "    X_test_mlp = X_test_a[mlp_cols]\n",
    "    sim_df['prob_E_1.5'] = modelE_p15.predict_proba(X_test_mlp)[:, 1]\n",
    "    \n",
    "    # Model F (Transformer)\n",
    "    test_values_scaled_trans = scaler_transformer.transform(test_values_extended.reshape(-1, 1))\n",
    "    X_trans, _, _, _ = create_sequences(test_values_scaled_trans, seq_len)\n",
    "    preds_f = model_transformer.predict(X_trans)\n",
    "    sim_df['prob_F_1.5'] = preds_f[0].flatten()[:min_len]\n",
    "\n",
    "    # Model B (k-NN)\n",
    "    preds_b_sim = []\n",
    "    pca_obj = locals().get('pca', None)\n",
    "    for idx in sim_df.index:\n",
    "        pat = create_pattern_vector(df['value'].values, idx)\n",
    "        if pat is not None:\n",
    "            p15, _, _ = predict_model_b(nbrs, pca_obj, targets, pat)\n",
    "            preds_b_sim.append(p15)\n",
    "        else:\n",
    "            preds_b_sim.append(0.5)\n",
    "    sim_df['prob_B_1.5'] = preds_b_sim\n",
    "    \n",
    "    # --- ENSEMBLE (Meta-Learner) ---\n",
    "    print(\"Ensemble (Meta-Learner) tahmin ediyor...\")\n",
    "    hmm_sim = hmm_states_mapped[test_start_idx : test_start_idx + len(sim_df)]\n",
    "    \n",
    "    meta_X_sim = prepare_meta_features(\n",
    "        sim_df['prob_A_1.5'], \n",
    "        sim_df['prob_B_1.5'], \n",
    "        sim_df['prob_C_1.5'], \n",
    "        sim_df['prob_D_1.5'], \n",
    "        sim_df['prob_E_1.5'], \n",
    "        hmm_sim,\n",
    "        values=sim_df['value'].values,\n",
    "        preds_transformer=sim_df['prob_F_1.5']\n",
    "    )\n",
    "    \n",
    "    sim_df['prob_Ensemble_1.5'] = predict_meta(meta_model, meta_scaler, meta_X_sim)\n",
    "    \n",
    "    # --- RL AGENT SİMÜLASYONU ---\n",
    "    print(\"\\n>>> RL Agent (The Strategist) Simülasyonu <<<\")\n",
    "    \n",
    "    balance = 1000.0\n",
    "    initial_balance = 1000.0\n",
    "    history_balance = [balance]\n",
    "    \n",
    "    # Anomaly Detector için son 300 veriyi tutmamız lazım\n",
    "    window_data = list(df['value'].values[test_start_idx-300 : test_start_idx])\n",
    "    \n",
    "    for i in range(len(sim_df)):\n",
    "        row = sim_df.iloc[i]\n",
    "        true_val = row['value']\n",
    "        \n",
    "        # 1. Anomaly Check (The Shield)\n",
    "        current_window = window_data[-300:]\n",
    "        score, _ = check_anomaly(anomaly_model, current_window)\n",
    "        \n",
    "        if score == -1:\n",
    "            # Anomaly Detected! Pass.\n",
    "            action = 0 # Pass\n",
    "        else:\n",
    "            # 2. RL Agent Decision (The Strategist)\n",
    "            recent_trend = np.mean(window_data[-10:])\n",
    "            \n",
    "            action = predict_action(\n",
    "                rl_agent, \n",
    "                row['prob_Ensemble_1.5'], \n",
    "                0.0, \n",
    "                hmm_sim[i], \n",
    "                balance, \n",
    "                recent_trend,\n",
    "                initial_balance\n",
    "            )\n",
    "            \n",
    "        # Execute Action\n",
    "        bet = 10.0\n",
    "        if action == 1: # Bet 1.5x\n",
    "            if true_val >= 1.5:\n",
    "                balance += bet * 0.5\n",
    "            else:\n",
    "                balance -= bet\n",
    "        elif action == 2: # Bet 2.0x\n",
    "            if true_val >= 2.0:\n",
    "                balance += bet * 1.0\n",
    "            else:\n",
    "                balance -= bet\n",
    "        \n",
    "        history_balance.append(balance)\n",
    "        window_data.append(true_val)\n",
    "        \n",
    "    # Plot Balance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history_balance)\n",
    "    plt.title(\"RL Agent Balance History\")\n",
    "    plt.xlabel(\"Games\")\n",
    "    plt.ylabel(\"Balance\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final Balance: {balance:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Simülasyon için yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Modelleri Yedekle (Google Drive & Local)\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "from google.colab import drive, files\n",
    "\n",
    "print(\"--- Modeller Yedekleniyor ---\")\n",
    "\n",
    "# 1. Zip Oluştur\n",
    "print(\"Modeller sıkıştırılıyor (models.zip)...\")\n",
    "!zip -r models.zip models/\n",
    "\n",
    "# 2. Google Drive Yedekleme\n",
    "try:\n",
    "    print(\"Google Drive bağlanıyor...\")\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Zaman damgalı klasör adı\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    backup_folder_name = f\"JetX_Backup_{timestamp}\"\n",
    "    backup_path = f\"/content/drive/MyDrive/{backup_folder_name}\"\n",
    "    \n",
    "    if not os.path.exists(backup_path):\n",
    "        os.makedirs(backup_path)\n",
    "        print(f\"Yedekleme klasörü oluşturuldu: {backup_path}\")\n",
    "        \n",
    "    # Kopyala\n",
    "    shutil.copy(\"models.zip\", f\"{backup_path}/models.zip\")\n",
    "    print(f\"✅ BAŞARILI: Modeller Drive'a yedeklendi -> {backup_path}/models.zip\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ UYARI: Google Drive yedeklemesi başarısız oldu: {e}\")\n",
    "    print(\"Yerel indirme deneniyor...\")\n",
    "\n",
    "# 3. Yerel İndirme (Her durumda sunulur)\n",
    "try:\n",
    "    files.download('models.zip')\n",
    "    print(\"✅ İndirme başlatıldı.\")\n",
    "except Exception as e:\n",
    "    print(f\"İndirme hatası: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}