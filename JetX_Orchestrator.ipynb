{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Proje Kurulumu ve G√ºncelleme (Colab stok + eksikler)\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Google Colab ortamƒ± algƒ±landƒ±.\")\n",
    "    repo_url = \"https://github.com/onndd/newproje.git\"\n",
    "    project_dir = \"/content/newproje\"\n",
    "\n",
    "    if os.path.exists(project_dir):\n",
    "        %cd {project_dir}\n",
    "        !git pull\n",
    "    else:\n",
    "        !git clone {repo_url}\n",
    "        %cd {project_dir}\n",
    "\n",
    "    print(\"Gerekli k√ºt√ºphaneler y√ºkleniyor...\")\n",
    "\n",
    "    # AGGRESSIVE FIX: Clean old numpy/pandas first\n",
    "    print(\"Cleaning conflicting libraries...\")\n",
    "    try:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"numpy\", \"pandas\"], check=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Uninstall notice (safe to ignore): {e}\")\n",
    "\n",
    "    print(\"Installing compatible numpy and pandas...\")\n",
    "    # Force reinstall with specific versions\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2.0\", \"pandas==2.2.2\", \"--force-reinstall\", \"--no-deps\"])\n",
    "    \n",
    "    # Re-install dependencies needed that might have been removed or need check\n",
    "    packages = [\n",
    "        \"catboost==1.2.8\",\n",
    "        \"lightgbm==4.6.0\",\n",
    "        \"hmmlearn==0.3.2\",\n",
    "        \"optuna==3.5.0\",\n",
    "        \"streamlit==1.29.0\",\n",
    "        \"stable-baselines3==2.2.1\",\n",
    "        \"shimmy==1.3.0\",\n",
    "        \"gymnasium==0.29.1\"\n",
    "    ]\n",
    "    \n",
    "    for pkg in packages:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "            print(f\"Successfully installed {pkg}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå ERROR installing {pkg}: {e}\")\n",
    "\n",
    "    if project_dir not in sys.path:\n",
    "        sys.path.append(project_dir)\n",
    "    \n",
    "    print(\"\\n‚úÖ KURULUM TAMAMLANDI.\")\n",
    "    print(\"üõëüõë √áOK √ñNEMLƒ∞: ≈ûƒ∞MDƒ∞ 'RUNTIME > RESTART SESSION' YAPMALISINIZ üõëüõë\")\n",
    "    print(\"     (Aksi takdirde 'numpy.dtype size changed' hatasƒ± gitmeyecektir.)\")\n",
    "else:\n",
    "    print(\"Local environment.\")\n",
    "    project_dir = os.getcwd()\n",
    "    if project_dir not in sys.path:\n",
    "        sys.path.append(project_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Veri Y√ºkleme ve 3'l√º Ayrƒ±m (3-Way Split)\n",
    "import sys\n",
    "import os\n",
    "if '/content/newproje' not in sys.path:\n",
    "    sys.path.append('/content/newproje')\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jetx_project.data_loader import load_data\n",
    "from jetx_project.config import DB_PATH, PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "print(\"Veri y√ºkleniyor...\")\n",
    "try:\n",
    "    df = load_data(DB_PATH)\n",
    "    print(f\"Toplam {len(df)} veri y√ºklendi.\")\n",
    "    \n",
    "    # --- 3-WAY SPLIT STRATEJƒ∞Sƒ∞ ---\n",
    "    # 1. Base Train (%70): Temel modelleri eƒüitmek i√ßin.\n",
    "    # 2. Meta Train (%15): Temel modellerin tahminlerini alƒ±p Meta-Learner'ƒ± eƒüitmek i√ßin.\n",
    "    # 3. Test (%15): Final sim√ºlasyon ve deƒüerlendirme i√ßin (Hi√ßbir model burayƒ± g√∂rmez).\n",
    "    \n",
    "    total_len = len(df)\n",
    "    base_train_end = int(total_len * 0.70)\n",
    "    meta_train_end = int(total_len * 0.85)\n",
    "    \n",
    "    print(f\"--- Veri Ayrƒ±mƒ± ---\")\n",
    "    print(f\"Base Train: 0 - {base_train_end} ({base_train_end} √∂rnek)\")\n",
    "    print(f\"Meta Train: {base_train_end} - {meta_train_end} ({meta_train_end - base_train_end} √∂rnek)\")\n",
    "    print(f\"Test (Sim): {meta_train_end} - {total_len} ({total_len - meta_train_end} √∂rnek)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Hata: {e}\")\n",
    "    print(\"L√ºtfen jetx.db dosyasƒ±nƒ±n proje klas√∂r√ºnde olduƒüundan emin olun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5. Anomaly Detector (The Shield) Eƒüitimi\n",
    "from jetx_project.model_anomaly import train_anomaly_detector, save_anomaly_detector\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"Anomaly Detector (The Shield) eƒüitiliyor...\")\n",
    "    # Base Train verisiyle eƒüit\n",
    "    train_values_anomaly = df['value'].values[:base_train_end]\n",
    "    anomaly_model = train_anomaly_detector(train_values_anomaly)\n",
    "    save_anomaly_detector(anomaly_model, output_dir='models')\n",
    "    print(\"Anomaly Detector tamamlandƒ±.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Psikolojik Modeller (HMM) Eƒüitimi\n",
    "print(\"Psikolojik Modeller (HMM) eƒüitiliyor...\")\n",
    "\n",
    "# Fallback: If hmmlearn is missing, install it here.\n",
    "try:\n",
    "    from hmmlearn import hmm\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è hmmlearn bulunamadƒ±, ≈üimdi y√ºkleniyor...\")\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"hmmlearn==0.3.2\"])\n",
    "    print(\"hmmlearn y√ºklendi.\")\n",
    "\n",
    "from jetx_project.model_hmm import train_categorical_hmm, predict_categorical_hmm_states_causal, save_hmm_model\n",
    "\n",
    "if 'df' in locals() and len(df) > 0:\n",
    "    # Sadece Base Train verisiyle eƒüit\n",
    "    train_values = df['value'].values[:base_train_end]\n",
    "    \n",
    "    # HMM Eƒüitimi (Categorical)\n",
    "    hmm_model, hmm_map, hmm_bins = train_categorical_hmm(train_values)\n",
    "    save_hmm_model(hmm_model, hmm_map, bins=hmm_bins, output_dir='models')\n",
    "    \n",
    "    # T√ºm veri seti i√ßin durum tahmini (Feature olarak kullanƒ±lacak)\n",
    "    # CAUSAL PREDICTION: Geleceƒüi g√∂rmeden, kayan pencere ile tahmin et.\n",
    "    all_values = df['value'].values\n",
    "    hmm_states_mapped = predict_categorical_hmm_states_causal(hmm_model, all_values, hmm_map, bins=hmm_bins, window_size=200)\n",
    "    \n",
    "    print(\"HMM Durumlarƒ± (Causal) t√ºm veri i√ßin belirlendi.\")\n",
    "    print(f\"Durum Daƒüƒ±lƒ±mƒ±: {np.bincount(hmm_states_mapped)}\")\n",
    "else:\n",
    "    print(\"Veri yok, HMM eƒüitilemedi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222fbbe",
   "metadata": {},
   "source": [
    "## üêØ MODEL A: CatBoost (The Hunter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.0. Model A (CatBoost) - √ñZELLƒ∞K √áIKARIMI üìä\n",
    "# BU H√úCRE √áOK √ñNEMLƒ∞Dƒ∞R. X_a (Eƒüitim Verisi) Burada Olu≈üur. \n",
    "# √áalƒ±≈ütƒ±rmazsanƒ±z sonraki adƒ±mlar \"X_a not found\" hatasƒ± verir.\n",
    "\n",
    "from jetx_project.model_a import prepare_model_a_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if 'df' in locals():\n",
    "    print(\"üêØ √ñzellikler √áƒ±karƒ±lƒ±yor (Feature Extraction)...\")\n",
    "    \n",
    "    # HMM Stateleri (Eƒüer HMM √ßalƒ±≈ütƒ±ysa, yoksa None)\n",
    "    # HMM h√ºcresinden 'hmm_states' gelmeli.\n",
    "    # G√ºvenlik i√ßin kontrol:\n",
    "\n",
    "    # --- MODIFIED HMM BRIDGE ---\n",
    "    # Global namespace'ten HMM sonu√ßlarƒ±nƒ± √ßekiyoruz\n",
    "    if 'hmm_states_causal' in globals():\n",
    "        hmm_states_mapped = hmm_states_causal\n",
    "        print(\"‚úÖ HMM Durumlarƒ± (causal) ba≈üarƒ±yla y√ºklendi.\")\n",
    "    elif 'hmm_states' in globals():\n",
    "        hmm_states_mapped = hmm_states\n",
    "        print(\"‚úÖ HMM Durumlarƒ± (standart) ba≈üarƒ±yla y√ºklendi.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è HMM Stateleri bulunamadƒ±, varsayƒ±lan (0) kullanƒ±lacak.\")\n",
    "        hmm_states_mapped = np.zeros(len(df))\n",
    "    # ---------------------------\n",
    "\n",
    "        \n",
    "    # Veri Hazƒ±rlƒ±ƒüƒ±\n",
    "    # X_a: √ñzellik Matrisi\n",
    "    # y_p15_a: Hedef (1.50x √ºst√º m√º?)\n",
    "    # y_p3_a: Hedef (3.00x √ºst√º m√º?)\n",
    "    # y_x_a: Hedef (Crash deƒüeri)\n",
    "    \n",
    "    X_a, y_p15_a, y_p3_a, y_x_a = prepare_model_a_data(df['value'].values, hmm_states_mapped, start_index=50)\n",
    "    \n",
    "    # Veri Sƒ±nƒ±rlarƒ±nƒ± Belirle (Optimization ve Training i√ßin)\n",
    "    # Train / Test ayrƒ±mƒ± zaten indekslerle yapƒ±lacak ama X_a t√ºm seti kapsar.\n",
    "    \n",
    "    print(f\"‚úÖ X_a Olu≈üturuldu. Boyut: {X_a.shape}\")\n",
    "    print(f\"   √ñrnek √ñzellikler: {list(X_a.columns[:5])}...\")\n",
    "else:\n",
    "    print(\"‚ùå 'df' bulunamadƒ±. L√ºtfen 2. H√ºcreyi (Veri Y√ºkleme) √ßalƒ±≈ütƒ±rƒ±n!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. Model A (CatBoost) - OPTƒ∞Mƒ∞ZASYON üõ†Ô∏è\n",
    "from jetx_project.optimization import optimize_catboost\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "if 'X_a' in locals():\n",
    "    print(\"üêØ Kaplan (CatBoost) i√ßin en iyi avlanma ayarlarƒ± aranƒ±yor...\")\n",
    "    \n",
    "    # 1. P1.5 (G√ºvenli Av)\n",
    "    print(\"   -> P1.5 Optimizasyonu...\")\n",
    "    best_params_catboost_p15 = optimize_catboost(X_a, y_p15_a, n_trials=15, scoring_params=PROFIT_SCORING_WEIGHTS)\n",
    "    \n",
    "    # 2. P3.0 (B√ºy√ºk Av)\n",
    "    print(\"   -> P3.0 Optimizasyonu...\")\n",
    "    best_params_catboost_p3 = optimize_catboost(X_a, y_p3_a, n_trials=15, scoring_params=PROFIT_SCORING_WEIGHTS_P3)\n",
    "    \n",
    "    print(f\"‚úÖ CatBoost Ayarlarƒ± Bulundu.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri (X_a) bulunamadƒ±. L√ºtfen Veri Y√ºkleme h√ºcresini √ßalƒ±≈ütƒ±rƒ±n.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28437ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Model A (CatBoost) - Eƒûƒ∞Tƒ∞M üèÉ\n",
    "from jetx_project.model_a import train_model_a, save_models\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "if 'best_params_catboost_p15' not in locals(): best_params_catboost_p15 = None\n",
    "if 'best_params_catboost_p3' not in locals(): best_params_catboost_p3 = None\n",
    "\n",
    "if 'X_a' in locals():\n",
    "    print(\"üêØ Kaplan (CatBoost) Eƒüitiliyor...\")\n",
    "    \n",
    "    modelA_p15, modelA_p3, modelA_x = train_model_a(\n",
    "        X_a, y_p15_a, y_p3_a, y_x_a, \n",
    "        params_p15=best_params_catboost_p15, \n",
    "        params_p3=best_params_catboost_p3,\n",
    "        scoring_params_p15=PROFIT_SCORING_WEIGHTS, \n",
    "        scoring_params_p3=PROFIT_SCORING_WEIGHTS_P3\n",
    "    )\n",
    "    \n",
    "    save_models(modelA_p15, modelA_p3, modelA_x, output_dir='models')\n",
    "    print(\"‚úÖ Model A Tamamlandƒ± ve Kaydedildi.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri (X_a) bulunamadƒ±.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75b518",
   "metadata": {},
   "source": [
    "## üëØ MODEL B: k-NN (The Mirror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model B (k-NN) - HAFIZA OLU≈ûTURMA üß†\n",
    "from jetx_project.model_b import build_memory, train_model_b, save_memory\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"üëØ ƒ∞kizler (k-NN) Hafƒ±zasƒ± Olu≈üturuluyor...\")\n",
    "    \n",
    "    train_values_b = df['value'].values[:base_train_end]\n",
    "    patterns, targets = build_memory(train_values_b)\n",
    "    nbrs, pca = train_model_b(patterns)\n",
    "    \n",
    "    save_memory(nbrs, pca, patterns, targets, output_dir='models')\n",
    "    print(\"‚úÖ Model B Hafƒ±zasƒ± Kaydedildi.\")\n",
    "else:\n",
    "    print(\"‚ùå Yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826687b0",
   "metadata": {},
   "source": [
    "## üîÆ MODEL C: LSTM (The Oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c7c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1. Model C (LSTM) - OPTƒ∞Mƒ∞ZASYON üõ†Ô∏è\n",
    "from jetx_project.optimization import optimize_lstm\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "import numpy as np\n",
    "\n",
    "if 'df' in locals():\n",
    "    print(\"üîÆ Kahin (LSTM) i√ßin zaman dalgalarƒ± analiz ediliyor (Optuna)...\")\n",
    "    \n",
    "    train_values_base = df['value'].values[:base_train_end]\n",
    "    \n",
    "    # 1. P1.5 (Standard 1.50x Threshold)\n",
    "    print(\"   -> P1.5 i√ßin Derinlik Ayarlanƒ±yor...\")\n",
    "    best_params_lstm_p15 = optimize_lstm(train_values_base, n_trials=10, scoring_params=PROFIT_SCORING_WEIGHTS, target_threshold=1.50)\n",
    "    \n",
    "    # 2. P3.0 (High 3.00x Threshold)\n",
    "    print(\"   -> P3.0 i√ßin Derinlik Ayarlanƒ±yor (THRESHOLD=3.00)...\")\n",
    "    best_params_lstm_p3 = optimize_lstm(train_values_base, n_trials=10, scoring_params=PROFIT_SCORING_WEIGHTS_P3, target_threshold=3.00)\n",
    "\n",
    "    print(f\"‚úÖ LSTM Ayarlarƒ± Bulundu.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri bulunamadƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "# 7.2. Model C (LSTM) - Eƒûƒ∞Tƒ∞M üèÉ\n",
    "from jetx_project.model_lstm import train_model_lstm, save_lstm_models\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "if 'best_params_lstm_p15' not in locals(): best_params_lstm_p15 = None\n",
    "if 'best_params_lstm_p3' not in locals(): best_params_lstm_p3 = None\n",
    "\n",
    "if 'df' in locals():\n",
    "    print(\"üîÆ Kahin (LSTM) Eƒüitiliyor...\")\n",
    "    train_values_c = df['value'].values[:base_train_end]\n",
    "    \n",
    "    modelC_p15, modelC_p3, scaler_lstm = train_model_lstm(\n",
    "        train_values_c, \n",
    "        params_p15=best_params_lstm_p15, \n",
    "        params_p3=best_params_lstm_p3, \n",
    "        scoring_params_p15=PROFIT_SCORING_WEIGHTS, \n",
    "        scoring_params_p3=PROFIT_SCORING_WEIGHTS_P3\n",
    "    )\n",
    "    \n",
    "    save_lstm_models(modelC_p15, modelC_p3, scaler_lstm, output_dir='models')\n",
    "    print(\"‚úÖ Model C Tamamlandƒ±.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri yok.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163d3e7",
   "metadata": {},
   "source": [
    "## ‚ö° MODEL D: LightGBM (The Flash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65799d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1. Model D (LightGBM) - OPTƒ∞Mƒ∞ZASYON üõ†Ô∏è\n",
    "from jetx_project.optimization import optimize_lightgbm\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "if 'X_a' in locals():\n",
    "    print(\"‚ö° ≈ûim≈üek (LightGBM) hƒ±z ayarlarƒ± yapƒ±lƒ±yor...\")\n",
    "    \n",
    "    best_params_lgb_p15 = optimize_lightgbm(X_a, y_p15_a, n_trials=15, scoring_params=PROFIT_SCORING_WEIGHTS)\n",
    "    best_params_lgb_p3 = optimize_lightgbm(X_a, y_p3_a, n_trials=15, scoring_params=PROFIT_SCORING_WEIGHTS_P3)\n",
    "    \n",
    "    print(f\"‚úÖ LightGBM Ayarlarƒ± Bulundu.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7126e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2. Model D (LightGBM) - Eƒûƒ∞Tƒ∞M üèÉ\n",
    "from jetx_project.model_lightgbm import train_model_lightgbm, save_lightgbm_models\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "if 'best_params_lgb_p15' not in locals(): best_params_lgb_p15 = None\n",
    "if 'best_params_lgb_p3' not in locals(): best_params_lgb_p3 = None\n",
    "\n",
    "if 'X_a' in locals():\n",
    "    print(\"‚ö° ≈ûim≈üek (LightGBM) Eƒüitiliyor (√ñzellik Se√ßimi ƒ∞le)...\")\n",
    "    \n",
    "    modelD_p15, modelD_p3, selected_features = train_model_lightgbm(\n",
    "        X_a, y_p15_a, y_p3_a, \n",
    "        params_p15=best_params_lgb_p15, \n",
    "        params_p3=best_params_lgb_p3,\n",
    "        scoring_params_p15=PROFIT_SCORING_WEIGHTS, \n",
    "        scoring_params_p3=PROFIT_SCORING_WEIGHTS_P3\n",
    "    )\n",
    "    \n",
    "    print(f\"   -> Se√ßilen En ƒ∞yi √ñzellikler: {selected_features}\")\n",
    "    save_lightgbm_models(modelD_p15, modelD_p3, output_dir='models')\n",
    "    print(\"‚úÖ Model D Tamamlandƒ±.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri yok.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e431a",
   "metadata": {},
   "source": [
    "## üß† MODEL E: MLP (The Brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688998eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. Model E (MLP) - OPTƒ∞Mƒ∞ZASYON üõ†Ô∏è\n",
    "from jetx_project.optimization import optimize_mlp\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "if 'X_a' in locals():\n",
    "    print(\"üß† Beyin (MLP) sinir aƒülarƒ±nƒ± yapƒ±landƒ±rƒ±yor...\")\n",
    "    \n",
    "    best_params_mlp_p15 = optimize_mlp(X_a, y_p15_a, n_trials=30, scoring_params=PROFIT_SCORING_WEIGHTS)\n",
    "    best_params_mlp_p3 = optimize_mlp(X_a, y_p3_a, n_trials=30, scoring_params=PROFIT_SCORING_WEIGHTS_P3)\n",
    "    \n",
    "    print(f\"‚úÖ MLP Ayarlarƒ± Bulundu.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072674e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2. Model E (MLP) - Eƒûƒ∞Tƒ∞M üèÉ\n",
    "from jetx_project.model_mlp import train_model_mlp, save_mlp_models\n",
    "from jetx_project.config import PROFIT_SCORING_WEIGHTS, PROFIT_SCORING_WEIGHTS_P3\n",
    "\n",
    "if 'best_params_mlp_p15' not in locals(): best_params_mlp_p15 = None\n",
    "if 'best_params_mlp_p3' not in locals(): best_params_mlp_p3 = None\n",
    "\n",
    "if 'X_a' in locals():\n",
    "    print(\"üß† Beyin (MLP) Eƒüitiliyor...\")\n",
    "    \n",
    "    modelE_p15, modelE_p3, mlp_cols = train_model_mlp(\n",
    "        X_a, y_p15_a, y_p3_a, \n",
    "        params_p15=best_params_mlp_p15, \n",
    "        params_p3=best_params_mlp_p3,\n",
    "        scoring_params_p15=PROFIT_SCORING_WEIGHTS, \n",
    "        scoring_params_p3=PROFIT_SCORING_WEIGHTS_P3\n",
    "    )\n",
    "    \n",
    "    save_mlp_models(modelE_p15, modelE_p3, mlp_cols, output_dir='models')\n",
    "    print(\"‚úÖ Model E Tamamlandƒ±.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri yok.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d6c79",
   "metadata": {},
   "source": [
    "## ü§ñ MODEL F: Transformer (The Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.0. Model F (Transformer) - Eƒûƒ∞Tƒ∞M (Sabit Parametreler) ü§ñ\n",
    "# Not: Transformer √ßok maliyetli olduƒüu i√ßin Optuna optimizasyonu atlanmƒ±≈ütƒ±r.\n",
    "# Standart \"Attention\" mekanizmasƒ± ile eƒüitilir.\n",
    "\n",
    "from jetx_project.model_transformer import train_model_transformer, save_transformer_models\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Colab path fix check\n",
    "project_dir = '/content/newproje'\n",
    "if os.path.exists(project_dir) and project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "if os.path.exists(project_dir):\n",
    "    %cd /content/newproje\n",
    "\n",
    "if 'df' in locals() and len(df) > 500:\n",
    "    print(\"ü§ñ Robot (Transformer) Eƒüitiliyor...\")\n",
    "    \n",
    "    # Base Train verisi √ºzerinde eƒüitim\n",
    "    train_values_f = df['value'].values[:base_train_end]\n",
    "    \n",
    "    # Model Eƒüitimi (Sabit Ayarlar: 20 Epoch, 4 Heads)\n",
    "    model_transformer, scaler_transformer = train_model_transformer(train_values_f)\n",
    "    \n",
    "    save_transformer_models(model_transformer, scaler_transformer, output_dir='models')\n",
    "    print(\"‚úÖ Model F (Transformer) Tamamlandƒ±.\")\n",
    "else:\n",
    "    print(\"‚ùå Yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169c174",
   "metadata": {},
   "source": [
    "## üåä MODEL G: Fourier Ritim Analizi (The Wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21257477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5. Model G (Fourier) - Rƒ∞Tƒ∞M ANALƒ∞Zƒ∞ VE G√ñRSELLE≈ûTƒ∞RME üåä\n",
    "# Bu model \"eƒüitilmez\", matematiksel analiz yapar.\n",
    "# Burada sinyallerin g√ºc√ºn√º g√∂rselle≈ütiriyoruz.\n",
    "\n",
    "from jetx_project.model_fourier import FourierDetector\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if 'df' in locals():\n",
    "    print(\"üåä Piyasa Ritmi Analiz Ediliyor (Son 2000 el)...\")\n",
    "    \n",
    "    # Son verileri al\n",
    "    values = df['value'].values\n",
    "    \n",
    "    # Dedekt√∂r√º Ba≈ülat (√áoklu Pencere)\n",
    "    detector = FourierDetector(window_sizes=[64, 256, 1024])\n",
    "    \n",
    "    # Analiz Yap (Batch Mode)\n",
    "    fourier_df = detector.analyze_batch(values)\n",
    "    \n",
    "    # Son 500 eli √ßizdirelim\n",
    "    plot_len = 500\n",
    "    if len(fourier_df) > plot_len:\n",
    "        subset = fourier_df.iloc[-plot_len:]\n",
    "        \n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # 1. Ritim G√ºc√º Grafiƒüi\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(subset['fourier_strength_64'].values, label='Kƒ±sa Vade (Hƒ±zlƒ±)', alpha=0.6)\n",
    "        plt.plot(subset['fourier_strength_256'].values, label='Orta Vade (Ritim)', linewidth=2)\n",
    "        plt.plot(subset['fourier_strength_1024'].values, label='Uzun Vade (Trend)', linestyle='--')\n",
    "        plt.title('Piyasa Ritim G√ºc√º (Sinyal Netliƒüi)')\n",
    "        plt.ylabel('G√º√ß (0-1)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Faz Grafiƒüi (D√∂ng√º Nerede?)\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(subset['fourier_phase_256'].values, color='purple', label='D√∂ng√º Konumu (0=Dip, 1=Tepe)')\n",
    "        plt.axhline(0.9, color='red', linestyle=':', label='Zirve B√∂lgesi')\n",
    "        plt.title('D√∂ng√º Konumu (Faz)')\n",
    "        plt.ylabel('Faz')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úÖ Analiz Tamamlandƒ±. Sinyaller Meta-Learner'a g√∂nderilmeye hazƒ±r.\")\n",
    "        print(f\"Son Durum (Orta Vade): G√º√ß={subset['fourier_strength_256'].iloc[-1]:.2f}, Faz={subset['fourier_phase_256'].iloc[-1]:.2f}\")\n",
    "    else:\n",
    "        print(\"Grafik i√ßin yeterli veri yok.\")\n",
    "else:\n",
    "    print(\"‚ùå Veri bulunamadƒ±.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.5. ENSEMBLE STACKING (Meta-Learner) Eƒüitimi (Meta Train)\n",
    "from jetx_project.ensemble import prepare_meta_features, train_meta_learner, save_meta_learner, predict_meta\n",
    "from jetx_project.model_lstm import create_sequences\n",
    "# Reloading Imports (Safety Net)\n",
    "from jetx_project.model_a import load_models as load_model_a\n",
    "from jetx_project.model_b import load_memory as load_model_b\n",
    "from jetx_project.model_lstm import load_lstm_models\n",
    "from jetx_project.model_lightgbm import load_lightgbm_models\n",
    "from jetx_project.model_mlp import load_mlp_models\n",
    "from jetx_project.model_transformer import load_transformer_models\n",
    "from jetx_project.model_hmm import load_hmm_model, predict_categorical_hmm_states\n",
    "\n",
    "if 'df' in locals() and len(df) > 2000:\n",
    "    print(\"--- Ensemble Stacking (Meta-Learner) Eƒüitiliyor ---\")\n",
    "    \n",
    "    # --- SAFETY CHECK: RELOAD MODELS IF MISSING ---\n",
    "    # Colab runtime might have cleared variables. Reload from disk if needed.\n",
    "    if 'modelA_p15' not in locals():\n",
    "        print(\"‚ö†Ô∏è Variables missing. Reloading models from disk...\")\n",
    "        modelA_p15, modelA_p3, modelA_x = load_model_a('models')\n",
    "        \n",
    "        nbrs, pca, targets, _ = load_model_b('models') # patterns not needed for prediction\n",
    "        \n",
    "        modelC_p15, modelC_p3, scaler_lstm = load_lstm_models('models')\n",
    "        \n",
    "        modelD_p15, modelD_p3 = load_lightgbm_models('models')\n",
    "        \n",
    "        modelE_p15, modelE_p3, mlp_cols = load_mlp_models('models')\n",
    "        \n",
    "        model_transformer, scaler_transformer = load_transformer_models('models')\n",
    "        \n",
    "        hmm_model, hmm_map, hmm_bins = load_hmm_model('models')\n",
    "        # Re-calc hmm_states_mapped if not present\n",
    "        if 'hmm_states_mapped' not in locals():\n",
    "             print(\"Recalculating HMM states...\")\n",
    "             # Use categorical prediction since we loaded categorical model\n",
    "             # Note: The training cell used causal prediction, but for simple reload we can use standard\n",
    "             # or simple re-run causal if 'predict_categorical_hmm_states_causal' imported.\n",
    "             # For simplicity here, let's use the standard predict function or assume it's loaded.\n",
    "             # Better: just use what we have.\n",
    "             from jetx_project.model_hmm import predict_categorical_hmm_states_causal\n",
    "             all_values = df['value'].values\n",
    "             hmm_states_mapped = predict_categorical_hmm_states_causal(hmm_model, all_values, hmm_map, bins=hmm_bins, window_size=200)\n",
    "        \n",
    "        print(\"‚úÖ Models reloaded successfully.\")\n",
    "    # -----------------------------------------------\n",
    "\n",
    "    meta_start_idx = base_train_end\n",
    "    meta_end_idx = meta_train_end\n",
    "    print(f\"Meta Train Aralƒ±ƒüƒ±: {meta_start_idx} - {meta_end_idx}\")\n",
    "    # 1. Meta Train Seti ƒ∞√ßin Tahminleri Topla\n",
    "    # Model A (CatBoost)\n",
    "    from jetx_project.model_a import prepare_model_a_data\n",
    "    X_meta, _, _, _ = prepare_model_a_data(df['value'].values, hmm_states_mapped, start_index=meta_start_idx)\n",
    "    X_meta = X_meta[X_meta.index < meta_end_idx]\n",
    "    # Hedefler\n",
    "    y_meta_true_15 = (df['value'].values[meta_start_idx+1 : meta_end_idx+1] >= 1.5).astype(int)\n",
    "    # Uzunluk Kontrol√º\n",
    "    min_len = min(len(X_meta), len(y_meta_true_15))\n",
    "    X_meta = X_meta.iloc[:min_len]\n",
    "    y_meta_true_15 = y_meta_true_15[:min_len]\n",
    "    print(f\"Meta Train √ñrnek Sayƒ±sƒ±: {min_len}\")\n",
    "    preds_a_meta = modelA_p15.predict_proba(X_meta)[:, 1]\n",
    "    # Model B (k-NN)\n",
    "    preds_b_meta = []\n",
    "    from jetx_project.model_b import create_pattern_vector, predict_model_b\n",
    "    pca_obj = locals().get('pca', None) \n",
    "    for idx in X_meta.index:\n",
    "        pat = create_pattern_vector(df['value'].values, idx)\n",
    "        if pat is not None:\n",
    "            p15, _, _ = predict_model_b(nbrs, pca_obj, targets, pat)\n",
    "            preds_b_meta.append(p15)\n",
    "        else:\n",
    "            preds_b_meta.append(0.5)\n",
    "    preds_b_meta = np.array(preds_b_meta)\n",
    "    # Model C (LSTM)\n",
    "    seq_len = 200\n",
    "    lstm_start_offset = X_meta.index[0] - seq_len\n",
    "    lstm_end_offset = X_meta.index[-1]\n",
    "    meta_values_extended = df['value'].values[lstm_start_offset : lstm_end_offset + 1]\n",
    "    meta_values_scaled = scaler_lstm.transform(meta_values_extended.reshape(-1, 1))\n",
    "    X_lstm_meta, _, _, _ = create_sequences(meta_values_scaled, meta_values_extended, seq_len)\n",
    "    preds_c_meta = modelC_p15.predict(X_lstm_meta).flatten()\n",
    "    # Model D (LightGBM)\n",
    "    preds_d_meta = modelD_p15.predict_proba(X_meta)[:, 1]\n",
    "    # Model E (MLP)\n",
    "    X_meta_mlp = X_meta[mlp_cols]\n",
    "    preds_e_meta = modelE_p15.predict_proba(X_meta_mlp)[:, 1]\n",
    "    # Model F (Transformer)\n",
    "    meta_values_scaled_trans = scaler_transformer.transform(meta_values_extended.reshape(-1, 1))\n",
    "    X_trans_meta, _, _, _ = create_sequences(meta_values_scaled_trans, meta_values_extended, seq_len)\n",
    "    preds_f_meta = model_transformer.predict(X_trans_meta)\n",
    "    preds_f_meta_p15 = preds_f_meta[0].flatten()\n",
    "    # HMM States\n",
    "    hmm_meta = hmm_states_mapped[X_meta.index]\n",
    "    # 2. Meta-Features Hazƒ±rla\n",
    "    final_len = min(len(preds_a_meta), len(preds_b_meta), len(preds_c_meta), len(preds_d_meta), len(preds_e_meta), len(preds_f_meta_p15))\n",
    "    meta_values_raw = df['value'].values[meta_start_idx : meta_end_idx]\n",
    "    meta_values_raw = meta_values_raw[:final_len]\n",
    "    meta_X_train = prepare_meta_features(\n",
    "        preds_a_meta[:final_len], \n",
    "        preds_b_meta[:final_len], \n",
    "        preds_c_meta[:final_len], \n",
    "        preds_d_meta[:final_len], \n",
    "        preds_e_meta[:final_len], \n",
    "        hmm_meta[:final_len],\n",
    "        values=meta_values_raw,\n",
    "        preds_transformer=preds_f_meta_p15[:final_len]\n",
    "    )\n",
    "    y_meta_train = y_meta_true_15[:final_len]\n",
    "    # 3. Eƒüit\n",
    "    meta_model, meta_scaler = train_meta_learner(meta_X_train, y_meta_train)\n",
    "    save_meta_learner(meta_model, meta_scaler, output_dir='models')\n",
    "    print(\"Meta-Learner eƒüitildi ve kaydedildi.\")\n",
    "else:\n",
    "    print(\"Yeterli veri yok.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.8. RL Agent (The Strategist) Eƒüitimi\n",
    "from jetx_project.model_rl import train_rl_agent, save_rl_agent\n",
    "\n",
    "if 'meta_model' in locals():\n",
    "    print(\"RL Agent (The Strategist) eƒüitiliyor...\")\n",
    "    \n",
    "    # Meta-Learner tahminlerini alalƒ±m\n",
    "    meta_probs = predict_meta(meta_model, meta_scaler, meta_X_train)\n",
    "    \n",
    "    # DataFrame olu≈ütur\n",
    "    rl_train_df = pd.DataFrame({\n",
    "        'value': meta_values_raw, # Ger√ßek deƒüerler\n",
    "        'prob_1.5': meta_probs,\n",
    "        'hmm_state': hmm_meta[:final_len] # HMM durumlarƒ±\n",
    "    })\n",
    "    \n",
    "    rl_agent = train_rl_agent(rl_train_df)\n",
    "    save_rl_agent(rl_agent, output_dir='models')\n",
    "    print(\"RL Agent tamamlandƒ±.\")\n",
    "else:\n",
    "    print(\"Meta-Learner hazƒ±r deƒüil.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. B√úY√úK Fƒ∞NAL: Sim√ºlasyon (Test Seti √úzerinde)\n",
    "# Strateji G√ºncellemesi: Sadece E≈üik 0.70 (HMM Filtresi Yok)\n",
    "\n",
    "print(\"--- B√úY√úK Fƒ∞NAL: Sim√ºlasyon Ba≈ülƒ±yor (Test Seti) ---\")\n",
    "\n",
    "test_start_idx = meta_train_end\n",
    "test_end_idx = len(df) - 1\n",
    "test_len = test_end_idx - test_start_idx\n",
    "print(f\"Test Seti Ba≈ülangƒ±√ß: {test_start_idx}, Uzunluk: {test_len}\")\n",
    "\n",
    "if test_len > 200:\n",
    "    # Test verisini hazƒ±rla\n",
    "    sim_df = df.iloc[test_start_idx:test_end_idx].copy()\n",
    "    sim_df = sim_df.reset_index(drop=True)\n",
    "    \n",
    "    print(\"Modeller tahmin ediyor...\")\n",
    "    \n",
    "    # 1. Toplu Tahmin (Batch Prediction)\n",
    "    \n",
    "    # Model A (CatBoost)\n",
    "    from jetx_project.model_a import prepare_model_a_data\n",
    "    X_test_a, _, _, _ = prepare_model_a_data(df['value'].values, hmm_states_mapped, start_index=test_start_idx)\n",
    "    min_len = min(len(X_test_a), len(sim_df))\n",
    "    X_test_a = X_test_a.iloc[:min_len]\n",
    "    sim_df = sim_df.iloc[:min_len]\n",
    "    \n",
    "    sim_df['prob_A_1.5'] = modelA_p15.predict_proba(X_test_a)[:, 1]\n",
    "    \n",
    "    # Model B (k-NN) - VECTORIZED / BATCH\n",
    "    print(\"Model B (k-NN) Batch Prediction...\")\n",
    "    from jetx_project.model_b import create_pattern_vector, predict_model_b\n",
    "    pca_obj = locals().get('pca', None)\n",
    "    \n",
    "    patterns_list = []\n",
    "    valid_indices = []\n",
    "    for idx in X_test_a.index:\n",
    "        pat = create_pattern_vector(df['value'].values, idx)\n",
    "        if pat is not None:\n",
    "            patterns_list.append(pat)\n",
    "            valid_indices.append(len(patterns_list)-1)\n",
    "        else:\n",
    "            patterns_list.append(np.zeros(300))\n",
    "            \n",
    "    import numpy as np\n",
    "    X_patterns = np.array(patterns_list)\n",
    "    preds_b_p15, _, _ = predict_model_b(nbrs, pca_obj, targets, X_patterns)\n",
    "    sim_df['prob_B_1.5'] = preds_b_p15\n",
    "    \n",
    "    # Model C (LSTM)\n",
    "    seq_len = 200\n",
    "    lstm_start = X_test_a.index[0] - seq_len\n",
    "    test_values_extended = df['value'].values[lstm_start : X_test_a.index[-1] + 1]\n",
    "    test_values_scaled = scaler_lstm.transform(test_values_extended.reshape(-1, 1))\n",
    "    X_lstm, _, _, _ = create_sequences(test_values_scaled, test_values_extended, seq_len)\n",
    "    sim_df['prob_C_1.5'] = modelC_p15.predict(X_lstm, verbose=0).flatten()[:min_len]\n",
    "    \n",
    "    # Model D (LightGBM)\n",
    "    sim_df['prob_D_1.5'] = modelD_p15.predict_proba(X_test_a)[:, 1]\n",
    "    \n",
    "    # Model E (MLP)\n",
    "    X_test_mlp = X_test_a[mlp_cols]\n",
    "    sim_df['prob_E_1.5'] = modelE_p15.predict_proba(X_test_mlp)[:, 1]\n",
    "\n",
    "    # Model F (Transformer)\n",
    "    meta_values_scaled_trans = scaler_transformer.transform(test_values_extended.reshape(-1, 1))\n",
    "    X_trans_test, _, _, _ = create_sequences(meta_values_scaled_trans, test_values_extended, seq_len)\n",
    "    preds_f_test = model_transformer.predict(X_trans_test, verbose=0)\n",
    "    sim_df['prob_F_1.5'] = preds_f_test[0].flatten()[:min_len]\n",
    "    \n",
    "    # HMM Feature for Filter (G√∂zlem i√ßin tutuyoruz ama ≈üart deƒüil)\n",
    "    hmm_test = hmm_states_mapped[X_test_a.index]\n",
    "    sim_df['hmm_state'] = hmm_test\n",
    "    \n",
    "    # 2. Meta-Learner Tahmini (+ Fourier G)\n",
    "    print(\"   -> Model G (Fourier) dahil ediliyor...\")\n",
    "    from jetx_project.ensemble import prepare_meta_features, predict_meta\n",
    "    \n",
    "    meta_X_test = prepare_meta_features(\n",
    "        sim_df['prob_A_1.5'].values,\n",
    "        sim_df['prob_B_1.5'].values,\n",
    "        sim_df['prob_C_1.5'].values,\n",
    "        sim_df['prob_D_1.5'].values,\n",
    "        sim_df['prob_E_1.5'].values,\n",
    "        hmm_test,\n",
    "        values=sim_df['value'].values,\n",
    "        preds_transformer=sim_df['prob_F_1.5'].values\n",
    "    )\n",
    "    \n",
    "    sim_df['meta_prob'] = meta_model.predict_proba(meta_X_test)[:, 1]\n",
    "    \n",
    "    # 3. Sim√ºlasyon\n",
    "    initial_balance = 1000\n",
    "    balance = initial_balance\n",
    "    balance_history = [balance]\n",
    "    bet_amount = 10\n",
    "    \n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    \n",
    "    print(\"\\n--- OPTƒ∞Mƒ∞ZE EDƒ∞LMƒ∞≈û Sim√ºlasyon Sonu√ßlarƒ± ---\")\n",
    "    print(\"Kurallar: G√ºven > 0.70 (HMM Filtresi YOK)\")\n",
    "    \n",
    "    for i, row in sim_df.iterrows():\n",
    "        prob = row['meta_prob']\n",
    "        real_outcome = row['value']\n",
    "        \n",
    "        # Strateji: Sadece Prob > 0.70\n",
    "        if prob > 0.70:\n",
    "            if real_outcome >= 1.50:\n",
    "                profit = bet_amount * 0.50\n",
    "                balance += profit\n",
    "                wins += 1\n",
    "            else:\n",
    "                balance -= bet_amount\n",
    "                losses += 1\n",
    "        \n",
    "        balance_history.append(balance)\n",
    "        \n",
    "    print(f\"Ba≈ülangƒ±√ß Kasa: {initial_balance}\")\n",
    "    print(f\"Biti≈ü Kasa: {balance:.2f}\")\n",
    "    print(f\"Kar/Zarar: {balance - initial_balance:.2f}\")\n",
    "    print(f\"Toplam Bahis: {wins + losses}\")\n",
    "    print(f\"Kazanma Oranƒ±: {wins / (wins + losses) if (wins+losses) > 0 else 0:.2%}\")\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(balance_history)\n",
    "    plt.title('Sim√ºlasyon Kasa Grafiƒüi (E≈üik 0.70)')\n",
    "    plt.xlabel('Oyun Sayƒ±sƒ±')\n",
    "    plt.ylabel('Bakiye')\n",
    "    plt.axhline(y=initial_balance, color='r', linestyle='--')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Test seti √ßok kƒ±sa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Modelleri Yedekle (Google Drive & Local)\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "from google.colab import drive, files\n",
    "\n",
    "print(\"--- Modeller Yedekleniyor ---\")\n",
    "\n",
    "# 1. Zip Olu≈ütur\n",
    "print(\"Modeller sƒ±kƒ±≈ütƒ±rƒ±lƒ±yor (models.zip)...\")\n",
    "!zip -r models.zip models/\n",
    "\n",
    "# 2. Google Drive Yedekleme\n",
    "try:\n",
    "    print(\"Google Drive baƒülanƒ±yor...\")\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Zaman damgalƒ± klas√∂r adƒ±\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    backup_folder_name = f\"JetX_Backup_{timestamp}\"\n",
    "    backup_path = f\"/content/drive/MyDrive/{backup_folder_name}\"\n",
    "    \n",
    "    if not os.path.exists(backup_path):\n",
    "        os.makedirs(backup_path)\n",
    "        print(f\"Yedekleme klas√∂r√º olu≈üturuldu: {backup_path}\")\n",
    "        \n",
    "    # Kopyala\n",
    "    shutil.copy(\"models.zip\", f\"{backup_path}/models.zip\")\n",
    "    print(f\"‚úÖ BA≈ûARILI: Modeller Drive'a yedeklendi -> {backup_path}/models.zip\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è UYARI: Google Drive yedeklemesi ba≈üarƒ±sƒ±z oldu: {e}\")\n",
    "    print(\"Yerel indirme deneniyor...\")\n",
    "\n",
    "# 3. Yerel ƒ∞ndirme (Her durumda sunulur)\n",
    "try:\n",
    "    files.download('models.zip')\n",
    "    print(\"‚úÖ ƒ∞ndirme ba≈ülatƒ±ldƒ±.\")\n",
    "except Exception as e:\n",
    "    print(f\"ƒ∞ndirme hatasƒ±: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
